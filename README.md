# Clawbot Cage — OpenClaw + Ollama (Docker Compose)

Run [OpenClaw](https://github.com/openclaw/openclaw) with fully local open-source models via Ollama. No API keys, no cloud dependencies. One command to start.

An interactive setup wizard walks you through model selection, network isolation, GPU setup, and channel configuration.

## Quick Start

```bash
git clone git@github.com:Todmy/clawbot-cage.git
cd clawbot-cage
./start.sh
```

The wizard will ask you to configure:

```
1. Model         — pick a model (5 options with size/RAM hints)
2. Network       — isolated (no internet) or internet (for messaging channels)
   2b. Web Search — allow the agent to search the web (only if internet)
3. Channels      — Telegram / Discord / WhatsApp setup (only if internet)
4. Dashboard     — localhost only or LAN-accessible
5. Platform/GPU  — macOS native Ollama (Metal) / Linux NVIDIA GPU / CPU
```

Then it builds, pulls the model, and starts everything. Dashboard at http://localhost:18789/.

### Non-interactive mode

For CI/automation, skip the wizard with env vars:

```bash
CLAWBOT_NON_INTERACTIVE=1 \
OLLAMA_MODEL=qwen2.5-coder:7b \
NETWORK_MODE=isolated \
./start.sh
```

## Security Model

Hardened by default. Multiple layers protect against prompt injection and host compromise.

**Network isolation (default: no internet)**
- Gateway and Ollama on an internal Docker network with no internet access.
- Ports bound to `127.0.0.1` — not exposed to LAN (configurable in wizard).
- Internet access explicitly opt-in via wizard or compose overlay.

**Container hardening**
- `read_only: true` — container root filesystem is immutable.
- `cap_drop: ALL` — all Linux capabilities removed, no privilege escalation.
- `no-new-privileges` — blocks setuid/setgid binaries.
- `tmpfs` for `/tmp` and cache — ephemeral, size-limited, not on host.
- Gateway runs as non-root user (`node`, uid 1000).

**Host filesystem protection**
- `config/` mounted **read-only** into the gateway — prompt injection cannot rewrite config, model settings, or system prompts.
- `workspace/` is the only writable host mount. Don't put secrets here.
- CLI runs on-demand only (not a persistent service).

**Agent sandbox (defense in depth)**
- Tool execution (shell, file ops) runs in throwaway Docker containers with `network: "none"`, read-only root, all capabilities dropped.
- Even if a prompt injection succeeds, the command runs in a sandbox with no network and no host access.

### What's still exposed

- Agent can read/write files in `./workspace/`.
- Gateway accepts connections on `127.0.0.1:18789` (local only, token-protected).
- When internet mode is enabled: gateway has outbound access (needed for channels). Agent sandbox still has no network.

## Prerequisites

- Git
- Docker and Docker Compose v2+
- ~25GB disk for the model + ~5GB for the OpenClaw image
- 32GB+ RAM recommended (CPU) or NVIDIA GPU with 20GB+ VRAM

## Project Structure

```
clawbot-cage/
├── .env                         # Generated by wizard (token, ports, mode)
├── .env.example                 # Template
├── docker-compose.yml           # Base: Ollama + gateway + CLI (isolated network)
├── docker-compose.internet.yml  # Overlay: adds internet access for gateway
├── docker-compose.gpu.yml       # Overlay: enables NVIDIA GPU for Ollama
├── config/
│   └── openclaw.json            # Generated by wizard (model, provider, sandbox)
├── workspace/                   # Agent workspace (created on first run)
├── start.sh                     # Setup wizard + build + launch
├── README.md
└── openclaw/                    # Cloned OpenClaw source (git-ignored, build context)
```

## Configuration

### .env

Generated by the wizard. Key variables:

| Variable | Default | Description |
|---|---|---|
| `OPENCLAW_GATEWAY_TOKEN` | auto-generated | Auth token for gateway access |
| `OPENCLAW_GATEWAY_PORT` | `18789` | Gateway HTTP/WS port |
| `OPENCLAW_BIND_HOST` | `127.0.0.1` | Bind address (`0.0.0.0` for LAN) |
| `OLLAMA_MODEL` | `qwen2.5-coder:32b` | Model to pull during setup |
| `NETWORK_MODE` | `isolated` | `isolated` or `internet` |
| `ENABLE_GPU` | `false` | `true` to enable NVIDIA GPU overlay |

### Switching Models

Easiest: rerun `./start.sh` — the wizard regenerates config.

Or manually edit `config/openclaw.json` (three places):

1. `agents.defaults.model.primary`
2. `agents.defaults.models` (allowlist)
3. `models.providers.ollama.models[0].id`

Then pull and restart:

```bash
docker compose exec ollama ollama pull <model-name>
docker compose restart openclaw-gateway
```

| Model | Ollama ID | Size | Notes |
|---|---|---|---|
| Qwen 2.5 Coder 32B | `qwen2.5-coder:32b` | ~20GB | Best coding quality |
| Qwen 2.5 Coder 7B | `qwen2.5-coder:7b` | ~4.5GB | Lightweight, 8GB RAM enough |
| Llama 3.3 70B | `llama3.3:70b` | ~40GB | Strong general model |
| Mistral Small 24B | `mistral-small:24b` | ~14GB | Balanced quality/size |
| DeepSeek Coder V2 | `deepseek-coder-v2:16b` | ~9GB | Solid coding model |

### GPU Support (Linux + NVIDIA)

The wizard auto-detects NVIDIA GPUs and offers to enable acceleration. Alternatively, apply the overlay manually:

```bash
docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d ollama
```

Requires [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html).

### macOS (Apple Silicon)

The wizard detects macOS and offers native Ollama mode (recommended). This uses the Metal GPU backend for inference instead of CPU-only Docker.

If configuring manually:

1. Install: `brew install ollama`
2. Start: `ollama serve`
3. Pull model: `ollama pull qwen2.5-coder:32b`
4. In `config/openclaw.json`, set `baseUrl` to `http://host.docker.internal:11434/v1`
5. Start gateway only: `docker compose up -d openclaw-gateway`

### Network Modes

| Mode | Internet | Use case |
|---|---|---|
| `isolated` (default) | No | Dashboard only. Most secure. |
| `internet` | Yes | Messaging channels (Telegram, Discord, WhatsApp), web search. |

Switch at any time:

```bash
# Enable internet
docker compose -f docker-compose.yml -f docker-compose.internet.yml up -d openclaw-gateway

# Back to isolated
docker compose up -d openclaw-gateway
```

## Adding Channels

The wizard can set up one channel during install. To add more later:

```bash
# WhatsApp (QR code scan)
docker compose --profile cli run --rm openclaw-cli channels login

# Telegram (get token from @BotFather)
docker compose --profile cli run --rm openclaw-cli channels add --channel telegram --token "<token>"

# Discord (get token from Developer Portal)
docker compose --profile cli run --rm openclaw-cli channels add --channel discord --token "<token>"
```

Requires internet mode.

## Common Commands

```bash
# View logs
docker compose logs -f openclaw-gateway

# Restart after config change
docker compose restart openclaw-gateway

# Stop everything
docker compose down

# Stop and remove all data (models, workspace)
docker compose down -v

# List loaded models
docker compose exec ollama ollama list

# Pull additional model
docker compose exec ollama ollama pull <model>

# Health check
docker compose exec openclaw-gateway node dist/index.mjs health --token "$OPENCLAW_GATEWAY_TOKEN"

# Rerun wizard
./start.sh
```

## Troubleshooting

**Gateway won't start / config validation error:**
```bash
docker compose --profile cli run --rm openclaw-cli doctor
docker compose --profile cli run --rm openclaw-cli doctor --fix
```

**Model not responding:**
```bash
docker compose exec ollama curl -s http://localhost:11434/v1/models
docker compose exec ollama ollama list
```

**Out of memory:** Rerun `./start.sh` and pick a smaller model.

**Slow inference on macOS Docker:** Rerun `./start.sh` and choose "Native" Ollama.

**Ports not reachable from LAN:** By design. Rerun `./start.sh` and pick "LAN" bind, or change `OPENCLAW_BIND_HOST=0.0.0.0` in `.env`.

## License

This configuration overlay is MIT. OpenClaw itself is [MIT licensed](https://github.com/openclaw/openclaw/blob/main/LICENSE).
